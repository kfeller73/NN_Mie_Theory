{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0286f920-46b4-45aa-afb4-d841a47bff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e98bd9-63cb-435b-b1ed-aa6b9d383922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.151192399859552 432.2319848111658 2.0809360292189343\n",
      " 2.2461137115316888e-06 4.593880688016641]\n",
      "(52400, 360)\n"
     ]
    }
   ],
   "source": [
    "def extract(file):\n",
    "    ang = np.linspace(-180*0.999, 180*0.999, 360) * np.pi/180\n",
    "    y = []\n",
    "    data = np.array(pd.read_csv(file))\n",
    "    x = data[:,:5].reshape(-1,5)\n",
    "    for i in range(len(x)):\n",
    "        y_i = list(data[i,5][1:-2].split(','))\n",
    "        y.append(y_i)\n",
    "    y = np.array(y).astype(float)\n",
    "    return x,y\n",
    "\n",
    "x0,y0 = extract(f'Phase_NN_data_jup0.csv')\n",
    "x1,y1 = extract(f'Phase_NN_data_jup1.csv')\n",
    "x2,y2 = extract(f'Phase_NN_data_jup2.csv')\n",
    "x = np.vstack((x0,x1,x2))\n",
    "y = np.vstack((y0,y1,y2))\n",
    "print(x[0])\n",
    "print(y.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f015c1f-d9e7-444f-8d13-e9fd92537635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sin data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27bfafef-cee5-49fe-8269-a5c85bc1ea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████▉                                       | 256/500 [06:33<06:14,  1.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KEYTON~1\\AppData\\Local\\Temp/ipykernel_3672/3556737113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ls = [5,360,360,360,360,360,360,360,360]\n",
    "        self.l1 = nn.Linear(ls[0],ls[1]) \n",
    "        self.l2 = nn.Linear(ls[1],ls[2]) \n",
    "        self.l3 = nn.Linear(ls[2],ls[3])\n",
    "        self.l4 = nn.Linear(ls[3],ls[4])\n",
    "        self.l5 = nn.Linear(ls[4],ls[5])\n",
    "        self.l6 = nn.Linear(ls[5],ls[6])\n",
    "        self.l7 = nn.Linear(ls[6],ls[7])\n",
    "        self.l8 = nn.Linear(ls[7],ls[8])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = F.relu(self.l5(x))\n",
    "        x = F.relu(self.l6(x))\n",
    "        x = F.relu(self.l7(x))\n",
    "        x = self.l8(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net().to(device)        \n",
    "# Hyper-parameters \n",
    "test_pct = 0.1\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 1000\n",
    "learning_rate = 0.00001\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "#loss_function = nn.L1Loss()\n",
    "val_size = int(len(x) * test_pct)\n",
    "### add in randomization of index\n",
    "shuf = np.random.permutation(len(x))\n",
    "x = x[shuf].astype(np.float32)\n",
    "y = y[shuf].astype(np.float32)\n",
    "\n",
    "X = torch.from_numpy(x)\n",
    "Y = torch.from_numpy(np.log(y))\n",
    "\n",
    "trainX = X[:-val_size].to(device)   \n",
    "trainY = Y[:-val_size].to(device)   \n",
    "testX = X[-val_size:].to(device)   \n",
    "testY = Y[-val_size:].to(device)  \n",
    "\n",
    "# Train the model\n",
    "eplos = []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for i in range(0, len(trainX), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "\n",
    "        batch_X = trainX[i:i+BATCH_SIZE]\n",
    "        batch_y = trainY[i:i+BATCH_SIZE]\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        net.zero_grad()\n",
    "    eplos.append([epoch, loss])\n",
    "print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "eplos = torch.tensor(eplos).to('cpu')\n",
    "#eplos = np.array(eplos.to('cpu'))\n",
    "plt.plot(eplos[:,0],eplos[:,1])\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "217c8fba-1d05-49ab-b00b-d16810783f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x[1,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2f49f8a-03b8-44bc-b249-9d0218ff76f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -2.501\n",
      "epoch 1: w = 0.051, loss = 51.08811951\n",
      "epoch 11: w = 1.688, loss = 1.32177675\n",
      "epoch 21: w = 1.951, loss = 0.03420247\n",
      "epoch 31: w = 1.994, loss = 0.00088962\n",
      "epoch 41: w = 2.000, loss = 0.00002747\n",
      "epoch 51: w = 2.002, loss = 0.00000490\n",
      "epoch 61: w = 2.002, loss = 0.00000408\n",
      "epoch 71: w = 2.002, loss = 0.00000383\n",
      "epoch 81: w = 2.002, loss = 0.00000360\n",
      "epoch 91: w = 2.002, loss = 0.00000339\n",
      "Prediction after training: f(5) = 10.003\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "             \n",
    "    # gradients = backward pass\n",
    "    l.backward()\n",
    "             \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "             \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a8630-d12b-4252-a3ad-2221d9002abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6404c5-214b-4856-9cd5-7e4c3f19f228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
